"""contains all helper functions used for deap RNN evolution code - all functions
that are not evaluation or part of the evolutionary code
"""

import numpy as np
import torch

def list_to_matrices(weight_list, num_in, num_hid, num_out):
	"""takes a list of weight generated by deap and converts
	it into numpy arrays that can then be entered into PyTorch
	as the parameters for an RNN to judge its fitness

	Assumes each RNN always has two weight matrices
	"""
	
	# define size of first weight matrix
	# size of second weight matrix follows from this
	w1_size = (num_in + num_hid)*num_hid
	w1_bias_size = num_hid
	w2_size = (num_hid*num_out)
	w2_bias_size = num_out
	
	# separate each of the weight into separate numpy arrays
	# resized after - returned now as 1D arrays
	w1 = np.array(weight_list[: w1_size], copy=True)
	w1_bias = np.array(weight_list[w1_size: (w1_size + w1_bias_size)], copy=True)
	w2 = np.array(weight_list[(w1_size + w1_bias_size): (w1_size + w1_bias_size + w2_size)],
			copy=True)
	w2_bias = np.array(weight_list[(w1_size + w1_bias_size + w2_size): ], copy=True)
	
	return (w1, w1_bias, w2, w2_bias)

def inject_weights(rnn, w1, w1_bias, w2, w2_bias):
	"""method that takes a pytorch rnn and sets the
	weights of its two linear units equal to w1 and 
	w2 so that their fitness can be tested with the RNN
	"""
	
	# find needed shapes of weight matrices
	w1_shape = rnn.in2hid.weight.data.numpy().shape
	w1_bias_shape = rnn.in2hid.bias.data.numpy().shape
	w2_shape = rnn.hid2out.weight.data.numpy().shape
	w2_bias_shape = rnn.hid2out.bias.data.numpy().shape	

	# reshape matrices to the proper shape
	w1 = np.reshape(w1, w1_shape)
	w1_bias = np.reshape(w1_bias, w1_bias_shape)
	w2 = np.reshape(w2, w2_shape)
	w2_bias = np.reshape(w2_bias, w2_bias_shape)
	
	# convert numpy arrays to tensors
	w1 = torch.from_numpy(w1).float()
	w1_bias = torch.from_numpy(w1_bias).float()
	w2 = torch.from_numpy(w2).float()
	w2_bias = torch.from_numpy(w2_bias).float()
	
	# set weights within the rnn equal to w1 and w2
	# types of weights must be float to avoid error with double
	rnn.in2hid.weight.data = w1
	rnn.in2hid.bias.data = w1_bias
	rnn.hid2out.weight.data = w2
	rnn.hid2out.bias.data = w2_bias
	
	# set all tensors in the state array equal to new weights
	rnn.state_dict()['in2hid.weight'].copy_(w1)
	rnn.state_dict()['in2hid.bias'].copy_(w1_bias)
	rnn.state_dict()['hid2out.weight'].copy_(w2)
	rnn.state_dict()['hid2out.bias'].copy_(w2_bias)

	# return the rnn with newly set weights
	return rnn

def get_rnn_output(rnn, max_it, act_exp, verbose=False):
	"""takes rnn with current weights and gets all outputs
	for the associated output circle
		

	Parameters:
	rnn -- the rnn being used
	max_it -- the maximum number of discrete points in the helical shape
	sigmoid_exp -- constant to multiply numbers passed into output activation
`	"""
	
	# initialize all tracking values that are needed
	# to create a structure with the rnn
	theta_scale = 20.0
	radius_scale = 4.0
	hidden = torch.zeros(1, rnn.hidden_size)
	all_positions = []
	r = 0.0
	theta = 0.0
	dr = 0.0
	dt = 0.0
	curr_t = 0 # track current t so that it does not exceed max

	# run rnn until candidate structure reaches the origin
	while (curr_t < max_it):
		# add current position into structure history
		rnn_pos = (r, theta)
		all_positions.append(rnn_pos)

		# get input and activate rnn at current timestep
		# be sure to normalize inputs before they are passed into RNN
		rnn_input = [[dr, dt]]
		outs, hidden = rnn.forward(torch.Tensor(rnn_input), hidden, act_exp)
		dr, dt = outs.data[0][0].item(), outs.data[0][1].item()
		
		# thickness should be scaled to minimum thickness and avoid negative thickness
		# tanh has a minimum value of -1, so add this to thickness and the minimum value
		# must also scale range of tanh to output values from min to max thickness
		#thick += (1.0 + MIN_THICKNESS)*((MAX_THICKNESS - MIN_THICKNESS)/2.0)	
	
		# print information
		if verbose:
			print("Current R: {0}".format(str(r)))
			print("dR: {0}".format(str(dr)))
			print("dT: {0}".format(str(dt)))
			#print("Thick: {0}".format(str(thick)))

		# update the current position of the structure
		# outputs are scaled to make changes not as large
		r += dr#(dr/radius_scale)
		theta += dt#abs(dt)/theta_scale

		# increment the current time step
		curr_t += 1

	# append the last position into the list
	'''
	if(r < 0):
		r = 0
	'''
	all_positions.append((r, theta))

	return all_positions

def get_RNN_output_cartesian(rnn, max_y, max_x, max_t, act_exp, verbose=False):
	"""gets RNN output for the gear tooth evolution - output starts at (0,0)
	in cartesian coordinates and moves upwards until it reaches a maximum value
	of y, forming a unique path from bottom to top that will be used as the shape
	of the gear tooth"""

	# intialize all values that need to be tracked by rnn
	x = 0.0
	y = 0.0
	dx = 0.0
	dy = 0.0
	x_scale = 5.0
	y_scale = 5.0
	curr_t = 0

	# initialize hidden state
	hidden = torch.zeros(1, rnn.hidden_size)

	# track all positions of RNN
	all_positions = []
	
	# run RNN until output reaches desired y position
	while(y < max_y and curr_t < max_t):
		# add current position into the positions list
		rnn_pos = (x, y)
		all_positions.append(rnn_pos)

		# form input and activte the rnn
		rnn_input = [[dx, dy]]
		outs, hidden = rnn.forward(torch.Tensor(rnn_input), hidden, act_exp)
		dx, dy = outs.data[0][0].item(), outs.data[0][1].item()

		# print out the dx and dy if it is verbose
		if(verbose):
			print("X: {0}".format(str(x)))
			print("Y: {0}".format(str(y)))
			print("dx: {0}".format(str(dx)))
			print("dy: {0}".format(str(dy)))
			input()

		# update x and y pos with rnn output
		x += (dx/x_scale)
		# x must be kept from going much too far from center
		if(x < -max_x):
			x = -max_x
		elif(x > max_x):
			x = max_x
		y += abs(dy/y_scale) # y must always move upward ? or is it ok?
		curr_t += 1
	
	# append the last position of the RNN
	all_positions.append((x, y))
	return all_positions
