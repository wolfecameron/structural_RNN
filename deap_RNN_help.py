"""contains all helper functions used for deap RNN evolution code - all functions
that are not evaluation or part of the evolutionary code
"""

import numpy as np
import torch

from deap_RNN_config import MIN_THICKNESS, MAX_THICKNESS

def list_to_matrices(weight_list, num_in, num_hid, num_out):
	"""takes a list of weight generated by deap and converts
	it into numpy arrays that can then be entered into PyTorch
	as the parameters for an RNN to judge its fitness

	Assumes each RNN always has two weight matrices
	"""
	
	# define size of first weight matrix
	# size of second weight matrix follows from this
	w1_size = (num_in + num_hid)*num_hid
	w1_bias_size = num_hid
	w2_size = (num_hid*num_out)
	w2_bias_size = num_out
	
	# separate each of the weight into separate numpy arrays
	# resized after - returned now as 1D arrays
	w1 = np.array(weight_list[: w1_size], copy=True)
	w1_bias = np.array(weight_list[w1_size: (w1_size + w1_bias_size)], copy=True)
	w2 = np.array(weight_list[(w1_size + w1_bias_size): (w1_size + w1_bias_size + w2_size)],
			copy=True)
	w2_bias = np.array(weight_list[(w1_size + w1_bias_size + w2_size): ], copy=True)
	
	return (w1, w1_bias, w2, w2_bias)

def inject_weights(rnn, w1, w1_bias, w2, w2_bias):
	"""method that takes a pytorch rnn and sets the
	weights of its two linear units equal to w1 and 
	w2 so that their fitness can be tested with the RNN
	"""
	
	# find needed shapes of weight matrices
	w1_shape = rnn.in2hid.weight.data.numpy().shape
	w1_bias_shape = rnn.in2hid.bias.data.numpy().shape
	w2_shape = rnn.hid2out.weight.data.numpy().shape
	w2_bias_shape = rnn.hid2out.bias.data.numpy().shape	

	# reshape matrices to the proper shape
	w1 = np.reshape(w1, w1_shape)
	w1_bias = np.reshape(w1_bias, w1_bias_shape)
	w2 = np.reshape(w2, w2_shape)
	w2_bias = np.reshape(w2_bias, w2_bias_shape)
	
	# convert numpy arrays to tensors
	w1 = torch.from_numpy(w1).float()
	w1_bias = torch.from_numpy(w1_bias).float()
	w2 = torch.from_numpy(w2).float()
	w2_bias = torch.from_numpy(w2_bias).float()
	
	# set weights within the rnn equal to w1 and w2
	# types of weights must be float to avoid error with double
	rnn.in2hid.weight.data = w1
	rnn.in2hid.bias.data = w1_bias
	rnn.hid2out.weight.data = w2
	rnn.hid2out.bias.data = w2_bias
	
	# set all tensors in the state array equal to new weights
	rnn.state_dict()['in2hid.weight'].copy_(w1)
	rnn.state_dict()['in2hid.bias'].copy_(w1_bias)
	rnn.state_dict()['hid2out.weight'].copy_(w2)
	rnn.state_dict()['hid2out.bias'].copy_(w2_bias)

	# return the rnn with newly set weights
	return rnn

def get_rnn_output(rnn, radius, max_it, verbose=False):
	"""takes rnn with current weights and gets all outputs
	for the associated output circle
		

	Parameters:
	rnn -- the rnn being used
	max_it -- the maximum number of discrete points in the helical shape
	"""
	
	# initialize all tracking values that are needed
	# to create a structure with the rnn
	theta_scale = 4.0
	r = radius
	theta = 0.0
	hidden = torch.zeros(1, rnn.hidden_size)
	all_positions = []
	dr = 0.0
	dt = 0.0
	thick = 0.0
	curr_t = 0 # track current t so that it does not exceed max

	# run rnn until candidate structure reaches the origin
	while (r > 0 and curr_t < max_it):
		# add current position into structure history
		rnn_pos = (r, theta, thick)
		all_positions.append(rnn_pos)

		# get input and activate rnn at current timestep
		rnn_input = [[r, theta, thick, dr, dt]]
		outs, hidden = rnn.forward(torch.Tensor(rnn_input), hidden)
		dr, dt, thick = outs.data[0][0].item(), outs.data[0][1].item(), outs.data[0][2].item()
		
		# thickness should be scaled to minimum thickness and avoid negative thickness
		# tanh has a minimum value of -1, so add this to thickness and the minimum value
		# must also scale range of tanh to output values from min to max thickness
		thick += (1.0 + MIN_THICKNESS)*((MAX_THICKNESS - MIN_THICKNESS)/2.0)	
	
		# print information
		if verbose:
			print("Current R: {0}".format(str(r)))
			print("dR: {0}".format(str(dr)))
			print("dT: {0}".format(str(dt)))
			print("Thick: {0}".format(str(thick)))

		# update the current position of the structure
		r -= dr
		theta += abs(dt)/theta_scale

		# increment the current time step
		curr_t += 1

	# append the last position into the list
	if(r < 0):
		r = 0
	all_positions.append((r, theta, thick))

	return all_positions
